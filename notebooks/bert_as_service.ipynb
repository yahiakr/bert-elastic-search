{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-as-service.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fmfi71s-dVx"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_pbE5f3Gr4k"
      },
      "source": [
        "#Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtc2HhMr-xZX"
      },
      "source": [
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import (WEIGHTS_NAME,BertConfig, BertForMaskedLM, BertTokenizer)\n",
        "\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, config):\n",
        "        # self.num_words = config.get(\"num_words\", 20)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"using device: {self.device}\")\n",
        "\n",
        "        model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "        #model.load_state_dict(torch.load(\"./model/pytorch_model.bin\"))\n",
        "        #model.eval()\n",
        "        model.to(self.device)\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, text):\n",
        "        input_ids = torch.tensor(self.tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
        "        outputs = self.model(input_ids)\n",
        "\n",
        "        embeddings = outputs[1][-1]\n",
        "        embeddings = embeddings.detach().numpy()[0]\n",
        "\n",
        "        size = embeddings.shape[0]\n",
        "        sum_array = [sum(x) for x in zip(*embeddings)]\n",
        "        avg_array = [sum_array[i]/size for i in range(len(sum_array))]\n",
        "\n",
        "        return avg_array"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo7ZbrKIGwzf"
      },
      "source": [
        "#Creating the API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H3Px8ioDNRr"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T_276kOG97i"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run\n",
        "\n",
        "#Init the model\n",
        "predictor = EmbeddingGenerator({})\n",
        "\n",
        "\n",
        "@app.route(\"/embedding\", methods=['POST'])\n",
        "def predict():\n",
        "    posted_data = request.get_json()\n",
        "    context = posted_data['context']\n",
        "    result = predictor.predict(context)\n",
        "\n",
        "    return jsonify({\n",
        "        \"vector\" : result,\n",
        "    })\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Running Flask on Google Colab!</h1>\"\n",
        "  \n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}